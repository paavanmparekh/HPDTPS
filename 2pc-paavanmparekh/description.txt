Two-Phase Commit (2PC) Protocol Implementation Blueprint

1. Prepare Phase
Coordinator Cluster - Leader Node: When a cross-shard transfer request (s, r, amt) arrives at the coordinator cluster's leader (the cluster owning account s), the leader first validates that it can proceed. It checks that s is not currently locked by another transaction and that s has a sufficient balance (>= amt). If either check fails (e.g. the record is locked or insufficient funds), the leader does not proceed with 2PC - it simply aborts or skips the transaction (similar to how intra-shard transactions are skipped when locks are held). If both conditions are satisfied, the coordinator leader continues with the prepare phase:
	1. Lock the source record s: The leader acquires a lock on s to prevent other transactions from modifying it during the 2PC process. This lock will remain until the transaction's final outcome (commit or abort) is determined.
	2. Send PREPARE message (this is 2PC PREPARE not Paxos's Prepare) to participant leader: The coordinator leader constructs a prepare request message <PREPARE, t, m> (where t identifies the transaction (s,r,amt) and m is the client's original request or a digest) and sends it to the leader of the participant cluster (the cluster owning account r). This notifies the participant cluster that a cross-shard transaction is pending and asks it to prepare. The coordinator sets a timer waiting for a response (PREPARED or ABORT) from the participant leader.
	3. Replicate "Prepare" via Multi-Paxos in coordinator cluster: In parallel with sending PREPARE, the coordinator leader initiates a Multi-Paxos consensus instance within its own cluster to log and execute the prepare-phase action. The leader allocates a new sequence number s (for the cluster's log) for this transaction and uses its current Paxos ballot number b. It then broadcasts an accept proposal <ACCEPT, b, s, P, m> to all replicas in the coordinator cluster. Here, the special flag 'P' marks this consensus value as a Prepare-phase record for a cross-shard transaction. All nodes will treat this as a tentative commit of the transaction's first phase. The coordinator leader includes the transaction details (or a hash digest) in the proposal.
	4. Paxos agreement on prepare record: The coordinator cluster's replicas (including the leader) run the consensus protocol to agree on the prepare record. Upon receiving the ACCEPT(P) message, each follower node checks the ballot and, if valid, writes the proposal to its log (persistent write-ahead storage for Paxos) and sends an acknowledgement to the leader. Once the leader receives a majority of acknowledgements (in a 3-node cluster, majority = 2), the prepare record is considered chosen (decided). The coordinator leader can then broadcast a commit/decide message internally (or simply infer the decision from the majority) to finalize the log entry.
	5. Execute the operation on coordinator shard (tentatively): After consensus, the coordinator cluster commits the prepare-phase entry in its state machine. This triggers a tentative execution of the transaction's effect on shard s. Every node in the coordinator cluster now deducts the transfer amount from account s in its local database (effectively debiting the sender) and marks this change as part of an uncommitted 2PC transaction. The nodes record the before-image (original balance of s) in a write-ahead log (WAL) so that the change can be undone if the transaction aborts. The prepare-phase log entry (with tag 'P') is now durably stored on all replicas, indicating that the cluster has executed the debit and is awaiting the final outcome. The lock on s remains held in the coordinator's lock table during this period (preventing other transactions on s).
	6. Response handling: The coordinator leader now waits for the participant cluster's reply. If a <PREPARED> message arrives from the participant before the timeout, it means the participant is ready. If an <ABORT> arrives or a timeout occurs, the coordinator will move to the abort path (described in the Abort Phase section). The timeout for waiting on PREPARED should be tuned (e.g. a few hundred milliseconds); if it expires, the coordinator presumes the participant failed or cannot prepare, and will initiate an abort.

Coordinator Cluster - Follower Nodes: Each follower in the coordinator cluster plays its part in the Multi-Paxos consensus for the prepare phase. Upon receiving the leader's <ACCEPT, b, s, P, m> proposal, a follower node checks that it has not seen a higher ballot (ensuring the current leader's authority). It then tentatively logs the proposal in its Paxos log and sends an acknowledgment (acceptance) back to the leader. Once the prepare record is decided (the follower might learn of this via the leader's commit message or by noticing its own acceptance and knowledge of majority), the follower marks the transaction as prepared in its local log. It then executes the debit on record s in its own copy of the database, just as the leader did, and updates its WAL with the pre-transaction state. The follower also marks s as locked. At this point, the follower's state reflects that s has been decremented by amt (temporarily) and that a commit or rollback is pending. The lock on s prevents any new operation from altering s until this transaction finishes. All this state (log entry + WAL) is persistent, so even if the follower crashes, it can recover knowing a prepare-phase operation was done.

Participant Cluster - Leader Node: The participant cluster's leader receives the <PREPARE, t, m> request from the coordinator. This cluster is responsible for the receiver account r. The participant leader performs a similar check on r: it verifies r is not currently locked by another transaction (meaning r is available to update)[11]. There is no balance check needed on r for receiving funds, but if r were involved in another concurrent transfer, it must wait.
* If record r is free (no lock): the participant leader can proceed to prepare. It immediately locks r to reserve it for this transaction[11]. Then it initiates a Multi-Paxos instance within the participant cluster to replicate the prepare action, just like the coordinator did. It assigns a sequence number in its own log for this transaction and broadcasts ?ACCEPT, b, s, P, m? to its followers (using its current ballot number b and marking the proposal with 'P')[11]. The content m (transaction details or digest) should match the coordinator's, so all nodes have a record of the same transaction. The participant leader also sets a timer waiting for the coordinator's final decision (to eventually handle a coordinator failure).
The participant cluster then runs Paxos to decide this prepare record. Once a majority of participant nodes accept the proposal, the participant leader concludes the prepare phase was successful. It commits the prepare-phase entry on the participant shard: the leader sends a commit/decide message internally and all participant nodes (including the leader) apply the transaction's effect on r. Specifically, every replica credits the amount amt to account r in its database, increasing r's balance, under the assumption that the transfer will commit. This change is logged in each node's WAL before applying, so it can be undone if necessary[12]. The participant leader then informs the coordinator of a successful prepare by sending a ?PREPARED, t, m? message back to the coordinator's leader[13]. This PREPARED message signifies "we have tentatively committed our part and are ready to finalize." The participant leader continues to hold the lock on r until the final outcome.

* If record r is already locked: the participant cluster cannot even prepare this transaction. In this case, the participant leader immediately decides to abort the transaction on its side (since it cannot safely prepare). It initiates a Paxos consensus to log an abort record for this transaction: it broadcasts an accept with value 'A' - ?ACCEPT, b, s, A, m? - to replicate an abort decision in its cluster. (The 'A' flag indicates this value is an Abort record.) A majority of participants will accept this, and the leader then commits the abort in the log. Because no state changes were applied to r (the lock conflict prevented any update), there is nothing to undo - the abort log is mainly for record-keeping. The participant leader unlocks r (if it was momentarily considered, but in this scenario r was locked by another transaction, which will continue to hold it; the current transaction simply gives up). The leader sends an ?ABORT, t, m? message to the coordinator leader immediately to inform it that the transaction cannot proceed. After sending this abort message, the participant cluster is effectively done with this transaction - it will not execute any changes. (If the coordinator later also sends an abort command, the participant can ignore it since it already aborted.) The participant leader's abort decision is persisted in its log (with 'A'), so all participant nodes know the transaction outcome was abort.

Participant Cluster - Follower Nodes: Each follower in the participant cluster participates in the Paxos round initiated by the participant leader. Upon receiving the ?ACCEPT, b, s, P, m? for a prepare, a follower verifies the ballot and logs the proposal, then acknowledges it. Once the prepare value is chosen (decided by majority), the follower applies the changes: it updates account r by adding amt to the balance (tentatively) and writes a WAL entry recording the old balance of r[12]. The follower marks r as locked in its local lock table. Now r's new balance is visible in memory/storage, but the transaction is not yet fully committed. If the leader sends a ?PREPARED? confirmation to the coordinator, followers learn the prepare phase succeeded, but they will wait for a commit or abort entry to appear in the log before releasing the lock. In the abort-on-lock-conflict scenario, if followers get an ?ACCEPT, b, s, A, m?, they log and accept the abort proposal. Once decided, they simply mark the transaction aborted in their logs; no data change is made to r (since no credit was applied) and thus no WAL undo is needed. Followers understand from the abort log entry that the transaction is finished (with no effect on the database). Any lock on r related to this transaction (if it was ever acquired) can be considered released immediately.


2. Commit Phase
The commit phase is triggered once the prepare phase has succeeded on both clusters. In practice, this means the coordinator leader has received a PREPARED message from the participant leader, and the coordinator's own cluster has the prepare entry decided in its log. At this point, the coordinator knows both shards have applied the transaction changes tentatively and are holding locks. The system now finalizes the outcome by committing the transaction in both clusters (or aborting, if something went wrong).
Coordinator Cluster - Leader Node: Upon receiving the PREPARED confirmation from the participant, the coordinator leader begins the commit phase[14] (assuming it hasn't timed out or aborted already). The coordinator performs two key actions for commit:
1. Send COMMIT message to participant: The coordinator leader sends a ?COMMIT, t, m? message to the participant cluster's leader[15]. This instructs the participant to finalize the transaction. The commit message includes the transaction identifier (so the participant knows which prepare it corresponds to). The coordinator sets a timer here as well, expecting an acknowledgment from the participant after it commits the transaction (to handle the case of message loss or participant failure).
2. Replicate "Commit" via Multi-Paxos in coordinator cluster: The coordinator leader starts a second round of consensus in its own cluster to record the commit decision. It reuses the same sequence number that was used for the prepare phase, but now proposes a new value with a 'C' flag (Commit) for that slot[16]. In other words, the coordinator broadcasts ?ACCEPT, b, s, C, m? to all coordinator nodes, where s is the original sequence index of this transaction's log entry and b is the current ballot (often the same leader and ballot, unless leadership changed)[16]. Using the same sequence number ties the commit entry to the corresponding prepare entry; the two rounds are distinguished in the log by the phase marker P vs. C[17]. The Paxos followers will treat this as a new proposal (since the value has changed to 'C') but typically the same leader can drive it to consensus quickly. A majority acceptance of this commit proposal means the commit record is chosen for that log index.
3. Finalize commit in coordinator cluster: Once consensus is reached on the commit record, the coordinator cluster's transaction is fully committed. The leader (and followers) record an entry with 'C' in their logs for this transaction[16]. At this point, the earlier tentative changes to s become permanent. The coordinator leader instructs all replicas (if not already self-evident from Paxos) to release the lock on s, since the transaction succeeded[18]. Each node can now free s for other transactions. The WAL entries related to this transaction's prepare phase can be marked as resolved - since the transaction committed, no rollback is needed. The nodes may keep the WAL record for audit but it won't be used unless recovery processes need it for some reason. The coordinator leader will then notify the client of the outcome. It prepares a reply to the client indicating a commit success (the transfer was completed). This reply can be sent as soon as the commit is decided in the coordinator cluster's log, or it may wait for the participant's acknowledgment to ensure the participant cluster also committed (see below)[19]. In this design, the coordinator generally waits for the participant's ACK before confirming to the client, to ensure all shards are consistent, but it will eventually reply with commit even if the participant is temporarily unresponsive (since the decision is commit, the client must eventually know it succeeded).
4. Await acknowledgment: After sending the COMMIT message, the coordinator leader waits for an acknowledgment from the participant leader confirming that the participant cluster replicated the commit entry (i.e., finished its second phase)[20][21]. If this ACK is not received within a timeout, the coordinator will retry sending the commit message (see Timeout Handling). The coordinator does not abandon the commit decision even if the ACK is delayed; once commit is decided, it must ensure the participant eventually knows about it (to avoid one shard committing while the other is unaware). The coordinator may resend COMMIT periodically until the participant responds. After receiving the ACK, the coordinator leader's work for this transaction is complete. It logs the ACK (optionally) and any client response has likely been sent by this point.
Coordinator Cluster - Follower Nodes: When the coordinator leader proposes an ACCEPT(C) for the commit, each follower receives ?... C, m? and processes it through Paxos. Followers will see that this proposal uses the same sequence s as an existing prepare entry. Because Paxos ensures only one value can be chosen for a given sequence (with a given ballot or higher), the followers treat this as the next step in the multi-value consensus. Under stable leadership, a follower will accept the commit proposal if it hasn't seen a higher ballot, log the commit marker, and send an acknowledgment. Once a majority (including possibly the follower itself) accepts, the commit is decided. The follower then marks the transaction as committed in its log and updates the state accordingly. Since the actual money deduction on s was already applied during the prepare phase, there may be no new state change to apply on commit, other than finalizing the status. The follower knows the transaction is now fully committed, so it releases the lock on s (making s available again)[18]. It can also discard or finalize the WAL entry for this transaction - no rollback will be needed, so the WAL record can be considered completed. All coordinator followers now have two log entries (or a composite entry) for the transaction: one marked 'P' and one marked 'C'. This double entry in the datastore/log reflects that the prepare phase and commit phase both executed[22]. The system state on each coordinator node now has s debited by amt, and this change is permanent.
Participant Cluster - Leader Node: When the participant leader receives the ?COMMIT, t, m? message from the coordinator, it initiates the second phase on the participant side[20]. The participant leader performs the following:
1. Replicate "Commit" via Multi-Paxos in participant cluster: The participant leader starts a Paxos round to log the commit decision for this transaction in its cluster. It uses the same sequence number in the participant cluster's log that was used for the prepare entry (ensuring the prepare and commit entries are linked) and sends ?ACCEPT, b, s, C, m? to all participant nodes (using its current ballot)[20]. This proposal carries the 'C' flag to indicate a commit. The participant cluster already has a 'P' entry for this transaction; now it will agree on a 'C' entry for the same slot. Followers acknowledge the proposal, and once a majority accepts, the commit value is chosen for that log entry.
2. Finalize commit in participant cluster: After consensus, the participant leader commits the transaction in its cluster. All participant nodes record the 'C' entry in the log. At this point, the earlier credit to account r (applied during prepare) is confirmed as permanent. The participant leader instructs all nodes to release the lock on r[18], since the money has been successfully added and the transaction completed. The WAL entries that recorded the pre-commit state of r can now be finalized (no need to undo). There is typically no additional state change on commit, because the credit was already applied; the commit just finalizes it.
3. Send acknowledgment to coordinator: Once the commit entry is durably replicated and the participant cluster has unlocked r, the participant leader sends an ACK message back to the coordinator leader to confirm that the participant side second phase is done[20]. This acknowledgment informs the coordinator that "shard r has committed." The participant leader may include the transaction ID in the ACK for clarity. After this, the participant cluster's involvement in the transaction is complete.
Participant Cluster - Follower Nodes: Each follower in the participant cluster receives the ?ACCEPT, b, s, C, m? proposal from the participant leader. The followers process it as a Paxos proposal: they check ballots, log the commit proposal, and reply with acknowledgments. Once the commit value is decided by majority, each follower writes the commit entry to its log. The follower then knows the transaction outcome is commit. It releases the lock on r (since the funds added to r are now confirmed)[18]. The follower does not need to modify r's balance further (the balance was already updated during prepare). It can now consider the WAL entry for r: since the transaction committed, the WAL entry (which holds old value of r before the credit) is no longer needed for recovery, though it might be kept for a short period or purged. At this point, all participant replicas have the transaction marked committed in their logs and the cluster is consistent.
Finally, with both coordinator and participant clusters having logged the commit, the 2PC protocol has succeeded for this transaction. Each involved node has two log entries for the transaction (prepare and commit)[22], the locks on s and r are released, and the client will be notified of success.


3. Abort Phase
The abort phase can occur in several situations: the participant could signal an abort during prepare (e.g., record r was locked), the coordinator could decide to abort due to a timeout or failure, or consensus could fail in one of the clusters. In any case, an abort phase ensures that both clusters agree to roll back the transaction and release any locks, undoing any tentative changes. Below we detail the responsibilities for each side when an abort is initiated.
Coordinator Cluster - Leader Node: The coordinator leader initiates the abort phase if any abort condition arises. Common triggers include: (a) the coordinator receives an ABORT message from the participant (meaning the participant could not prepare, as in the locked-r scenario)[10]; (b) the participant never responded with PREPARED within the timeout (implying participant failure or message loss)[10]; or (c) the coordinator's own prepare-phase consensus was not successful (e.g., unable to get a majority, perhaps due to node failures)[10]. In these cases, the coordinator must abandon the transaction and ensure both shards do the same. The abort phase on the coordinator side involves:
1. Replicate "Abort" via Multi-Paxos in coordinator cluster: The coordinator leader starts a Paxos consensus round to log an abort decision for the transaction. It uses the same sequence number s that was assigned during the prepare phase for this transaction and proposes a value with flag 'A' (Abort). It sends ?ACCEPT, b, s, A, m? to all coordinator nodes[23]. The content m (transaction info) is the same, but now 'A' denotes that this log entry represents an abort outcome. Once a majority of followers accept this proposal, the abort value is chosen and logged in the coordinator cluster's datastore.
2. Finalize abort in coordinator cluster: After consensus, every coordinator node records an 'A' entry for the transaction, indicating it was aborted[23]. The coordinator leader (or Paxos decision) then instructs all nodes to undo the tentative changes that were made during the prepare phase. Specifically, since the coordinator shard had debited amt from s, each node now uses the WAL record to restore s's balance to its original value[9]. The debit applied earlier is rolled back, effectively canceling the transfer of funds out of s. This undo operation is done before releasing the lock, to ensure no other transaction sees an incorrect intermediate balance. After reverting the state, the coordinator nodes drop or mark the WAL entry as applied (the WAL entry served its purpose to undo). The coordinator leader and followers then release the lock on s[9], since the transaction is no longer in progress. Now s is free for other transactions, and its balance is as it was before the transfer attempt.
3. Send ABORT message to participant: The coordinator leader sends a ?ABORT, t, m? message to the participant cluster's leader to ensure the participant aborts as well[24]. This is done unless the coordinator knows the participant already aborted on its own. (The protocol specifies that the explicit abort message can be skipped if the participant "is no longer waiting for the outcome" because it aborted itself already[25]. For example, in the case where r was locked, the participant leader already sent an abort to the coordinator and has internally handled aborting.) In most other cases, the participant might be in a prepared state waiting for a commit/abort, so the coordinator's abort message is crucial to tell it to rollback. The coordinator sets a timer here waiting for an ACK from the participant that it has completed the abort.
4. Inform the client: The coordinator leader will eventually respond to the client with a failure/abort outcome. Typically, once the abort is logged in the coordinator cluster (and ideally after the participant has acknowledged), the leader sends a reply to the client indicating that the transaction did not commit. This lets the client know the transfer failed. The coordinator may do this immediately after initiating the abort (optimistically, assuming the participant will follow suit) or after receiving participant's ACK for extra safety. In either case, the client sees the transaction as aborted.
Coordinator Cluster - Follower Nodes: Upon receiving the coordinator leader's ?ACCEPT, b, s, A, m? proposal, each follower logs the proposed abort decision for the transaction and sends an acknowledgment. Once a majority is reached and the abort is decided, each follower writes the abort entry to its persistent log. The follower then proceeds to undo the transaction's effects on s*: it consults its WAL to get the original balance of *s (from before the prepare-phase deduction) and restores s to that value[9]. Essentially, the deducted amount amt is added back to s's balance on each replica. After the rollback, the follower releases the lock on s, since the transaction is finished (aborted)[9]. The abort entry in the log serves as a record that the prepare entry was not followed by a commit but instead was compensated by a rollback. The follower can now consider that transaction closed. If later a recovery or audit occurs, the pair of log entries ('P' and 'A') and the WAL contents can confirm that the transfer was voided.
Participant Cluster - Leader Node: The participant side will perform abort phase actions either proactively (if it initiated the abort during prepare) or reactively (if it receives an abort command from the coordinator). There are two main scenarios:
* Abort initiated during Prepare: This was described in the prepare phase: if r was locked, the participant leader already ran an abort consensus with an 'A' value and sent an ABORT message to the coordinator. In that scenario, the participant cluster is essentially already in abort phase during what would have been the prepare stage. The participant leader and followers have recorded the abort, and no changes to r were made, so nothing needed undoing. By the time the coordinator leader receives that abort message, it will (in commit/abort phase) initiate its own abort. The coordinator may or may not send back an ABORT command to the participant; if it does, the participant sees that it was already aborted and can ignore the duplicate instruction[25]. In summary, when participant aborts early, it considers the transaction done and frees r immediately. The participant leader in that case had locked nothing (or briefly considered r locked and released it), so r is available for other transactions once the abort is decided.
* Abort on coordinator's command: If the participant cluster had successfully prepared (i.e., it sent PREPARED) and is waiting for a decision, an abort may come from the coordinator (due to some failure elsewhere). When the participant leader receives a ?ABORT, t, m? message from the coordinator, it proceeds to abort the transaction on its side. The participant leader initiates a Paxos consensus to record the abort outcome: it sends ?ACCEPT, b, s, A, m? to all participant nodes (using the same log sequence s that was used for the prepare entry)[20]. The cluster reaches consensus on the abort value ('A'). Once decided, the participant leader performs the undo of the transaction's effects on r: it uses the WAL to subtract the previously credited amt from r's balance, restoring r to its original state (before the prepare)[9]. It then releases the lock on r[9], since the transaction will not commit. The participant leader sends an acknowledgment back to the coordinator indicating that the abort has been completed on its side (essentially an "Abort ACK"). After this, the participant cluster's state for that transaction is: a 'P' entry in the log (prepare) followed by an 'A' entry (abort) indicating it was rolled back, r is unchanged from before the transaction, and the WAL entry is used/cleared.
Participant Cluster - Follower Nodes: If the participant cluster aborted early (lock conflict case), the followers would have logged the abort in the prepare phase itself and no state change was applied, so followers simply saw the 'A' log and know the transaction is done (with no action needed on the data). If the abort comes in the second phase (from coordinator), each follower will receive the ?ACCEPT, b, s, A, m? proposal from the leader. They log and accept it, then after majority decision they commit the abort entry to their log. At that time, each follower reverts the tentative update to r: using its WAL record of r's old balance, it subtracts the credited amount out of r, effectively undoing the prepare-phase addition[9]. This returns r's balance to what it was pre-transaction. The follower then releases the lock on r. The WAL entry can be discarded after the undo, since the abort is finalized. At this point, all participant replicas concur that the transaction aborted and their state reflects no net change from the transaction. If the coordinator's abort message was redundant (in case they had already locally aborted), the followers might have already done these steps; in any event, the outcome is consistent: funds not transferred.
After an abort, the coordinator will not send a commit (obviously), and the client will be told the transaction failed. Both shards have either never changed (if aborted early) or have rolled back the changes (if aborted after prepare). The locks have been released and any partial effects undone via the WAL.
One additional note: if a timeout or failure occurs and the coordinator aborts, the participant may independently also time out and start an abort (see Timeout Handling below). The protocol is designed so that even if both sides initiate an abort concurrently, the end result is the same (abort), and the log entries on each side will both show abort. The coordinator's abort message in that case might cross with the participant's, but that's harmless since both agree to abort.


4. Timeout Handling
Distributed transactions involve waiting for messages and consensus steps, so timeouts are crucial to detect failures and prevent indefinite waiting. The 2PC protocol sets up timers at various points to handle delays or failures gracefully. Below are the main timeout scenarios and how the system should respond to each:
* Timeout waiting for PREPARED (Coordinator side, during prepare phase): After the coordinator leader sends the PREPARE message to the participant leader, it expects either a PREPARED or an ABORT reply. A timer (e.g. a few hundred milliseconds, depending on network conditions) starts when PREPARE is sent. If this timer expires before any reply is received from the participant, the coordinator assumes that the participant either did not receive the prepare request or has crashed/lagged. In response, the coordinator leader will treat this as a failed prepare. It initiates an abort of the transaction: the coordinator runs the abort consensus in its cluster and logs an abort entry (if not already logged)[10], then sends an ABORT message to the participant cluster[24]. This ensures that if the participant did get the prepare and was waiting, it will now abort as well. Essentially, a prepare-phase timeout on the coordinator triggers the Abort Phase as described above. The coordinator should also notify the client that the transaction didn't go through (after concluding the abort). If the participant's PREPARED message arrives after the coordinator has decided to abort (a late message), the coordinator will ignore it because the decision to abort is already made. The timeout duration should be set based on expected network latency and processing time; it should be long enough to avoid false aborts under normal conditions but short enough to not stall the system if the participant is down (for example, on the order of tens or hundreds of milliseconds).
* Timeout waiting for final COMMIT/ABORT decision (Participant side, after sending PREPARED): After the participant leader sends a PREPARED message to the coordinator, it expects either a COMMIT or ABORT command in reply to conclude the protocol. The participant should not wait forever for the coordinator (because the coordinator could crash after the prepare phase). Therefore, the participant leader also uses a timer once it has sent PREPARED. If this timer expires without receiving either a COMMIT or ABORT from the coordinator, the participant must assume the coordinator has failed or the message was lost. In a fail-stop model, a coordinator crash means no commit is coming unless a new coordinator leader decides one. The participant leader's safe course is to abort the transaction on its own after a timeout. In practice, upon timeout, the participant leader will initiate the abort consensus in its cluster (if not already done) - logging an 'A' for the transaction - and undoing any tentative update to r using the WAL, then releasing the lock on r. It may also send an ABORT message to the coordinator (or whoever is the new coordinator leader) as a courtesy, though if the coordinator is truly down, it won't be received. This self-abort ensures the participant's resources are freed. The timeout for this should generally be longer than the coordinator's prepare timeout, since the participant knows the coordinator might have crashed and could be recovering. For example, the participant might wait a bit longer than the coordinator's typical failover time. This prevents a situation where the coordinator is just slow to respond but not actually dead - however, since a commit decision would eventually come, the participant should ideally hold off aborting if there's a chance to still get a commit. If the participant does abort due to timeout, and later a COMMIT arrives (perhaps the coordinator recovered late or the message was delayed), the participant would reject it because it has already decided abort (the coordinator would then have to reconcile that, likely the transaction overall aborts). This scenario is undesirable but is resolved by ensuring timing parameters are well-chosen. The key point is that the participant will not block indefinitely: it will abort after a reasonable wait to avoid holding a lock forever.
* Timeout waiting for 2PC ACK (Coordinator side, after sending COMMIT/ABORT): After the coordinator leader dispatches a COMMIT or ABORT message to the participant leader, it expects an acknowledgment from the participant indicating that the second phase (commit or abort) has been completed on that side[21]. The coordinator sets a timer for this ACK. If the timer expires before an ACK is received, the coordinator assumes the participant might not have gotten the message or is slow to process it. In response, the coordinator will retransmit the same COMMIT or ABORT message to the participant[21]. The coordinator will continue to retry periodically until it gets the acknowledgment. Importantly, the coordinator does not change the outcome when a timeout happens here - if it decided commit, it remains committed and will keep trying to tell the participant to commit; switching to abort at this point is not allowed because that could violate atomicity. Similarly, if the outcome was abort, it keeps sending abort until confirmed (though if the participant was down and comes back, it might have already aborted itself). The retries should use either a fixed interval or an exponential backoff. For example, retry after 100ms, then 200ms, 400ms, etc., or some suitable schedule, to balance between quick recovery and not flooding the network. This mechanism covers scenarios like the participant leader crashing right after prepare: the new participant leader might come up and not know the outcome, but the coordinator's repeated commit/abort messages will ensure the new leader learns the outcome once it's up. There is typically no upper bound on how many retries - the coordinator will keep the transaction outcome in memory (or stable storage) and keep trying until an ACK is received, or until it's certain the participant has acted (for instance, if the participant sends a duplicate outcome message or if higher-level logic decides to stop). This guarantees eventual consistency between clusters.
* Timeout in Paxos consensus (intra-cluster): Within each cluster, the Multi-Paxos leader uses timeouts to detect lack of progress in the consensus protocol. For example, when the coordinator leader sent the ACCEPT, P message, it waits for a majority of acknowledgments. If some followers are down or slow and a majority isn't reached within a timeout, the leader might fail to gather quorum. In Paxos, the leader would then typically increase its ballot number and retry the proposal (possibly after a new prepare phase)[26]. The implementation from Project 1 likely has such a mechanism: the leader's timer triggers a ballot increment to attempt to regain quorum with a higher proposal number if needed. This ensures that transient issues or a follower crash don't stall consensus forever. However, if the reason for timeout is that too many nodes are down (e.g., in a 3-node cluster, 2 nodes are down so you can't get 2 ACKs), then consensus is impossible until at least one node recovers. The system should recognize this scenario. For the 2PC protocol, if a cluster cannot achieve consensus on a prepare or commit record due to insufficient quorum, the transaction cannot commit in that cluster. According to the project guidelines, "no consensus if too many nodes fail (disconnect)" is a possible scenario[27], meaning the system should handle it gracefully. In practice, the coordinator cluster's leader will timeout on the Paxos round and realize it's not getting a majority. It should then decide to abort the transaction (since it cannot safely replicate the prepare or commit). This corresponds to the coordinator "consensus not achieved" case that triggers an abort[10]. The coordinator would log an abort (if even that is possible - if a majority is down, even logging abort may require waiting for recovery; the coordinator might have to defer the final decision until quorum is back, effectively blocking the transaction). If the coordinator cannot even get quorum to log the abort, the system is stuck until some node recovers. Similarly, on the participant side, failure to get quorum for the prepare means the participant leader cannot even tell if it prepared or not - it might also have to abort once quorum is back. The practical approach is: as soon as a leader detects it cannot reach majority, it should signal an abort for the transaction (perhaps by a local flag) and, when quorum is re-established, finalize that abort through the log. During the period without quorum, the record s or r might remain locked in a limbo state (since one node alone cannot decide anything). To avoid a deadlock in the whole system, one might consider a coordinator timeout override: if the participant cluster doesn't respond at all (possibly due to no quorum), the coordinator aborts after a timeout as we said, which at least frees the coordinator shard. The participant shard might only have one node alive which can't decide the Paxos abort entry; in that case, the participant's lock on r might not have been acquired (if leader was down, maybe nothing happened), or if it was acquired by a now-failed leader, it's effectively released when that leader failed. So in extreme cases, progress halts until nodes recover. The main point is the implementation should handle timeout events at each step: Paxos timeouts trigger retries or aborts, 2PC message timeouts trigger retries or aborts, and no operation waits forever. Proper logging and timeouts ensure that if a node or message fails, the protocol either retries or aborts in a bounded time.
* Timeout and WAL usage: Whenever a timeout leads to an abort decision, nodes will use the WAL to roll back any changes made. The project specification explicitly notes that if an outcome is abort or if a timeout occurs, the WAL should be used to undo executed operations before releasing locks[9]. This implies the system equates a timeout-induced abort to an ordinary abort in terms of cleanup. Thus, all the undo steps described in the Abort Phase will occur on a timeout as well. The locks are only released after state has been restored to pre-transaction values, maintaining consistency.
To summarize, timers should be set for: waiting for PREPARED reply, waiting for coordinator's commit/abort (at participant), waiting for ACK after sending outcome, and Paxos consensus responses. Each timer expiration has a defined recovery action (retry or abort). Care must be taken to choose reasonable timeout durations (perhaps on the order of tens to a few hundreds of milliseconds, depending on expected environment latency) and to implement retransmission with backoff to avoid network congestion. Also, when a node is deliberately failed (via the testing interface), its timers must be stopped[26], as continuing to run could incorrectly trigger events when the node is supposed to be off (for example, increasing ballots or sending messages when it should be silent). Stopping the timer on a failed node prevents, say, a paused leader from erroneously starting new Paxos rounds that no one else will acknowledge[26].

5. Recovery Handling
The system is designed under a fail-stop model, meaning nodes may crash or disconnect, but do not perform arbitrary faulty actions. Recovery handling ensures that when a failed node comes back online, it can rejoin the protocol and the system can continue to operate correctly. Key aspects of recovery include resynchronizing state (via logs), handling any in-progress transactions, and cleaning up any remnants of the node's pre-failure state (like stale locks or timers).
Fail-Stop Failure Behavior: When a node fails (crashes or is manually failed via F(n_i) in the test input), it should be treated as completely unresponsive. The implementation should not actually kill the process, but logically isolate it: the node should stop processing incoming messages, stop sending any messages, and stop participating in Paxos or 2PC in any way[29]. Essentially, it becomes invisible to the rest of the system (other nodes will notice its absence when timeouts occur). Additionally, all timers on the failed node must be stopped[26]. This is important: if a Paxos leader fails and its election timer keeps ticking in the background, it might trigger and increment ballot numbers or send messages even though the node is "down", causing confusion. Stopping the timers ensures the failed node remains completely inert. The node should also stop writing to its log or WAL when "down" (no partial entries). From the perspective of other nodes, a failed node is just not responding. Other cluster members will detect the lack of response and may initiate leader re-election or abort transactions as described in Timeout Handling.
During the time a node is down, the system might reconfigure leadership. For example, if the coordinator leader fails mid-transaction, one of its followers will typically be elected as the new leader (assuming a majority of that cluster is still up) so that the cluster can continue Paxos operations. The new leader might complete or abort any transactions that were in flight. Similarly, if a participant leader fails, a new participant leader will take over if possible. The design from Project 1 likely included leader election in Multi-Paxos, which will carry over here to ensure that each cluster can continue to make progress with a new leader after a failure.
Node Recovery Process: When a node is instructed to recover (R(n_i) in the tests) or otherwise restarts after a crash, it needs to rejoin its cluster and synchronize its state. The recovery involves several steps:
* Reinitialize and Catch Up Logs: The recovering node should first ensure its in-memory state (data store, lock table, etc.) is initialized from durable storage. Critically, it should have persisted its Paxos log and WAL to disk. Upon restart, it loads the latest state it knew. Because it was not active during its downtime, there may be log entries that the cluster decided in its absence. The recovering node must acquire those missing log entries to be up-to-date. Typically, this is done by communicating with the current cluster leader or other peers. For instance, the recovering node can ask for all log entries from a certain index onward, or the leader (upon noticing a follower with a lagging log) can send it the missing entries (this is akin to Paxos state transfer or the catch-up mechanism). In this project, it's implied that each node has a WAL and a commit log (datastore log). We saw that every node appends two entries per cross-shard transaction (prepare and commit/abort)[22]. On recovery, the node can compare its log with peers to find out what entries it missed and append them. This may involve running the Paxos leader's recovery protocol (e.g., the leader might re-propose committed values to the recovering node or send a snapshot). The implementation needs to include a way for a recovering node to synchronize; otherwise, the recovered node "will be unable to participate in the consensus process"[30] because it's out-of-date or has a skewed ballot. (Stopping the timer on fail helps prevent skewed ballots; now on recovery the node should probably reset its ballot to a default until it catches up.)
* Apply Missed Transactions: Once the recovering node obtains the missing log entries, it needs to apply all committed operations to its local database to catch up. For example, suppose a node in the coordinator cluster went down after the prepare phase of a transaction. While it was down, the coordinator's remaining nodes and the participant cluster might have decided to commit the transaction, and perhaps other transactions occurred too. The recovering node's log, once updated, will show for that transaction a 'P' entry (which it might have had or maybe not) and a 'C' entry (which it definitely missed if commit happened while it was down). The recovering node should now apply the commit: if it had previously applied the prepare (maybe it crashed after doing so, or maybe it never got the prepare if it failed earlier), it needs to ensure the final state reflects a committed transaction. Concretely, if it's a coordinator node and sees a commit entry, it must ensure account s is debited by amt (since commit means the debit stands). If it had already done the debit during prepare (and perhaps even written it to WAL), it should check that its database still has that debit (if it crashed after applying it, it might still be in its data; if it crashed before applying, its data might show the old balance - in which case it now needs to apply the debit because the transaction committed). By consulting the WAL and log, it can reconcile this. If instead the log shows an abort entry for that transaction, the recovering node must ensure that any partial change it made is undone. For instance, if it had deducted s during prepare, but then sees 'A', it must credit s back using the WAL record. If it crashed before undoing, now is the time to do it. Essentially, for each transaction in the log that has an outcome (commit or abort), the recovering node updates its state to match that outcome. This may involve re-running the operations or rollbacks as needed. The WAL provides the pre-transaction values to restore on abort cases[9]. The node should also update its lock table: any transaction that has a final outcome should no longer hold locks. If a transaction was mid-flight and unresolved when the node failed, that leads to the next point.
* Handling In-Progress Transactions: It's possible a node crashed while a 2PC was in progress (e.g., after sending PREPARE but before getting PREPARED, or after sending PREPARED but before getting commit). In such cases, when it recovers, it needs to determine the status of those transactions. The logs of the cluster will indicate if those transactions were committed or aborted in the meantime. If the cluster reached a decision without this node (which it can if a majority remained), the recovering node will simply see the decided value in the log and can apply it. If the transaction was not decided because the failure prevented quorum (e.g., coordinator leader and one follower went down, leaving only one node, so no majority), then upon recovery a new leader might now exist (or the recovering node might become leader) and needs to resolve that transaction. Typically, unresolved transactions after a failure are aborted. For example, if a coordinator leader crashed after sending prepare to participant but didn't finish logging the prepare entry, the new leader might not even have that request in its log (unless one follower got it). If the participant prepared and is waiting, the new coordinator leader may not know that. One robust approach is: participants that are prepared and don't hear commit will time out and abort themselves, as described. So, by the time the coordinator leader comes back, the participant might have aborted. The coordinator, not having a decided prepare, can safely abort too. During recovery, the node that comes up can check its WAL - if it has a WAL entry for a prepared transaction but sees no commit in log, it should assume an abort happened (or will happen) and proceed to undo if not already undone. It may also consult other nodes: e.g., a recovering coordinator might run a Paxos "preparation/leader election" phase where it learns there was an uncommitted proposal. Paxos dictates that if a value was accepted by some nodes with no decision, the new leader could attempt to repropose it. However, in 2PC's context, it's probably better to abort it to avoid uncertainty (since the participant might have aborted). The specifics might be complex, but in summary, any transaction that remains incomplete on recovery will be aborted by the new leader to free resources. The recovering node will abide by whatever the new leader does via the log.
* Releasing or Reinstating Locks: A recovering node must reconcile its lock table with the rest of the cluster. Because locks are not globally synchronized beyond the consensus logs, the node should derive lock status from the state of transactions. For any transaction that was prepared (P logged) but not yet committed or aborted in the log, that means the lock should still be considered held in the cluster. If the cluster as a whole hasn't decided, the new leader will decide abort, and that decision will release the lock. If the recovering node sees a 'P' without a matching 'C' or 'A', it knows that while it was down the transaction was left hanging (likely due to loss of quorum). In that case, the recovering node should treat the affected records (like s or r) as locked until it learns otherwise. Often, the new cluster leader upon regaining quorum will immediately push an abort for that entry, at which point the lock can be cleared. If the recovering node itself becomes leader and sees this situation, it should initiate an abort consensus for that entry. Conversely, if the log shows an abort or commit, the lock is no longer active. If the recovering node's local lock table had a lock (from before crash) that now no longer applies (because the transaction finished while it was down), it must not keep that lock. Thus, part of recovery is cleaning up obsolete locks. In practice, it might be easiest to reconstruct the lock table from the log: for each transaction in prepare state (P without C/A) still in the log, mark those items as locked; everything else is unlocked. This way, the recovered node's lock table is consistent with the cluster's reality.
* Resume Operation: After catching up the log and adjusting local state, the recovering node can safely rejoin normal processing. It will now participate in consensus protocols as a follower. If it was previously a leader, it won't automatically reclaim leadership (unless the current leader fails and Paxos elects it). One must also reset the node's Paxos timers and ballots appropriately. For instance, if the node's ballot number was high when it crashed (maybe due to a timer firing just as it failed), it should communicate in Paxos prepare phase to ensure it doesn't unintentionally disrupt the cluster. Typically, a recovering node might start with a fresh ballot (like 0 or some low number plus its ID) until it hears the current ballot from the leader. The key is that by stopping its timers when it failed, we avoided the scenario of an ever-increasing ballot, so recovery shouldn't introduce a ridiculously high ballot unless logic was wrong[26]. The node should be ready to accept new Paxos proposals and 2PC messages.
* Persistent Storage and WAL replay: Because each node writes crucial data to stable storage (the consensus log for decided entries, and the WAL for interim updates), recovery leverages these to restore state. The WAL contains uncommitted changes that were applied. If the log shows that a transaction aborted, the node uses the WAL to rollback those changes on recovery (if they weren't rolled back earlier)[9]. If the log shows commit, the WAL entries for that transaction can be discarded (or marked as committed) since no rollback is needed. If the node crashed before applying an operation but the cluster committed it, the recovering node will see a commit in the log and can safely apply the operation now (this is effectively a redo using the log, not an undo). If the node crashed after applying a prepare but before knowing the outcome, the WAL entry is still there; the log will now inform whether to undo or finalize it. Thus, the WAL and the log together allow the node to replay or rollback actions as needed to converge with the cluster's state.
In summary, on recovery, a node performs the following step-by-step: 1. Stop being "disconnected" and resume normal operation (open network connections, etc.). 2. Synchronize its Paxos log with the cluster (learn any entries it missed while offline). 3. Use the updated log to update the application state: - For each newly learned commit entry, apply the operation if not already done. - For each newly learned abort entry, ensure any partial operation is undone. - Clear or update WAL entries corresponding to those decisions. - Update the lock table to reflect only ongoing (not yet finalized) transactions, if any. 4. Rejoin the consensus as a follower. The node should be able to vote on new proposals and handle new 2PC messages now. 5. If this node was involved in an in-flight transaction at failure (e.g., it was the coordinator or participant leader that crashed mid-2PC), trust that the new leader took care of it (likely aborted) while it was down. If not, now that it's up, its cluster can reach a decision. The recovered node will cooperate in that process (e.g., if it becomes leader, it will abort such a transaction to resolve ambiguity).
The testing instructions specifically note that we can fail and recover any node and that the system should handle those cases[29]. For instance, a test might fail a participant leader after it sends PREPARED but before it commits. The coordinator will retry commit until the participant recovers. On recovery, the new participant leader (which might even be the recovered node itself) will receive the pending commit and then finalize it, sending an ACK. Our implementation must ensure that scenario works: the recovered node must catch up and then correctly process the commit message that the coordinator has been retrying. Another test might fail the coordinator leader during the protocol - the participant will eventually time out and abort. When the coordinator's cluster gets a new leader (or the old leader recovers), it should also abort and notify the participant (which might have already aborted). The recovered coordinator node would see that the transaction was aborted by its new leader (or decide to abort if it became leader).
Throughout recovery, data consistency and atomicity are preserved by the logs. The combination of writing both a prepare record and a commit/abort record in each shard's log ensures that after any failures, each node can determine whether a transaction was committed or aborted by examining its log (once up-to-date). Any side effects that were only partially done can be rolled back via WAL. No transaction should permanently lock a record due to a failure: either a commit or abort will appear in the logs once quorum is restored, and then the lock is released and any temporary changes either made durable (commit) or undone (abort)[9].
Finally, note that when a node fails, the test infrastructure may specify which nodes are considered "live" for each test scenario. After processing all transactions in a set, the system may be flushed/reset for the next set. But within a single run, recovery should allow the node to continue as part of the system. By implementing the above blueprint, a Go-based system would maintain correct behavior of the cross-shard 2PC on top of Multi-Paxos, even in the face of node crashes and recoveries. All responsibilities of leaders and followers in both coordinator and participant clusters are accounted for: messaging (REQUEST, PREPARE, PREPARED, COMMIT, ABORT, ACK), logging to persistent storage (consensus log with P/C/A markers, and WAL for data rollback), locking of records (and releasing locks on commit/abort), and Multi-Paxos consensus for fault-tolerant agreement on each phase's outcome. This blueprint can be used to guide an implementation to ensure every step is handled correctly according to the project requirements and the 2PC protocol described in Section 1.2 of the project PDF. The end result is a system where each cross-shard transaction either commits on both shards or aborts on both, even if failures occur, and no permanent divergence or deadlock occurs[27][31]. All decisions are replicated on a majority of nodes in each cluster, providing fault tolerance as well as the ability to recover the state after failures.